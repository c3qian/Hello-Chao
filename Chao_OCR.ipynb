{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chao OCR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9OHEzwXIvmCL",
        "YRYeN6RDuQdT"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3qian/Hello-Chao/blob/master/Chao_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "id": "XFfgMJlPuQcn",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Recurrent Neural Network (CRNN) for OCR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JgNYfx7uQcq",
        "colab_type": "text"
      },
      "source": [
        "Steps for OCR:\n",
        "\n",
        "1. Preprocessing Data\n",
        "2. Creating Network Architecture(CTC loss function)\n",
        "3. Training Model\n",
        "4. Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHEzwXIvmCL",
        "colab_type": "text"
      },
      "source": [
        "###Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgFIw1auQcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "74594013-3d51-47ad-b4f0-fbb29426068e"
      },
      "source": [
        "import os\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "\n",
        "from glob import glob\n",
        " \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 3406356911365021906\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 11641911842688634676\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 3531102088120465099\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11326753997\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 8831939635512936401\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpr242KuQc3",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9JmCRIVuQc6",
        "colab_type": "text"
      },
      "source": [
        "1. Download and unzip the dataset into a folder\n",
        "2. Preprocess the data: both inputs and outputs\n",
        "\n",
        "Input:\n",
        "\n",
        "*   Read the images and convert them into gray-scale images(why?)\n",
        "*   Reshape each image to size (128,32) (why?)\n",
        "*   Expand the dimension of the image from (128,32) to (128,32,1) (why?)\n",
        "*   Normalize the image pixel values by dividing it with 255\n",
        "\n",
        "Output:\n",
        "\n",
        "*   Read the image file names as the labels of that image\n",
        "*   Encode word into digits using a map (‘a’:0, ‘b’:1 …….. ‘z’:26 ......) e.g.  \"aabb\" -> [0,0,1,1]\n",
        "*   Find the maximum length among all words and pad every label to be the same size(max size) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJTiEllx815",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "77e8a85b-a63b-4795-cd97-fd756eea7f53"
      },
      "source": [
        "!wget https://transfer.sh/ZHMV4/dataset.zip\n",
        "!unzip dataset.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-13 15:56:00--  https://transfer.sh/ZHMV4/dataset.zip\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6534503 (6.2M) [application/zip]\n",
            "Saving to: ‘dataset.zip.2’\n",
            "\n",
            "dataset.zip.2       100%[===================>]   6.23M  2.60MB/s    in 2.4s    \n",
            "\n",
            "2019-07-13 15:56:07 (2.60 MB/s) - ‘dataset.zip.2’ saved [6534503/6534503]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "replace dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/dataset/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEj3uYCW_Lre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cac100c7-f313-41f5-db78-14c5defcca6d"
      },
      "source": [
        "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "char_list = string.ascii_letters+string.digits\n",
        "print('char_list:',char_list)\n",
        "print('total length:', len(char_list))\n",
        " \n",
        "# every word is encoded as a list of digits\n",
        "# the digit for each character is represented by the index\n",
        "# e.g. aabb -> [0,0,1,1], index of a is 0, index of b is 1\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for i in txt:\n",
        "      dig_lst.append(char_list.index(i))\n",
        "    return dig_lst"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_list: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "total length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djKsVr35uQc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'dataset/'\n",
        " \n",
        "#lists for training dataset\n",
        "train_x = []\n",
        "train_y = []\n",
        "train_x_len = []\n",
        "train_y_len = []\n",
        "orig_y = []\n",
        " \n",
        "#lists for validation dataset\n",
        "val_x = []\n",
        "val_y = []\n",
        "val_x_len = []\n",
        "val_y_len = []\n",
        "val_orig_y = []\n",
        " \n",
        "max_label_len = 0\n",
        " \n",
        "flag = 0\n",
        " \n",
        "for i, f_name in enumerate(glob(os.path.join(path,'*/*.jpg'))):\n",
        "    # read input image and convert into gray scale image\n",
        "    img = cv2.imread(f_name, 0)\n",
        "    #img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # convert each image of shape (32, 128, 1)\n",
        "    # hint: cv2.resize, np.expand_dims\n",
        "    \n",
        "    \n",
        "    \n",
        "   \n",
        "    img = cv2.resize(img, (128,32))\n",
        "    img = np.expand_dims(img, axis =2) \n",
        "    # Normalize each image\n",
        "    img = img/255\n",
        "\n",
        "    # get the text from the image\n",
        "    txt = os.path.basename(f_name).split('_')[1]\n",
        "\n",
        "    # compute maximum length of the text\n",
        "    if len(txt) > max_label_len:\n",
        "        max_label_len = len(txt)\n",
        "\n",
        "\n",
        "    # split the data into validation and training dataset as 1:9\n",
        "    if np.random.rand() < 0.1:     \n",
        "        val_x.append(img)\n",
        "        val_y.append(encode_to_labels(txt))\n",
        "        val_x_len.append(31)\n",
        "        val_y_len.append(len(txt))\n",
        "        val_orig_y.append(txt)  \n",
        "    else:\n",
        "        train_x.append(img)\n",
        "        train_y.append(encode_to_labels(txt)) \n",
        "        train_x_len.append(31)\n",
        "        train_y_len.append(len(txt))\n",
        "        orig_y.append(txt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqOzYAS5Jhp",
        "colab_type": "code",
        "outputId": "dad52127-0b9e-428f-9eb1-0e9754053208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "# Check your preprocessing results\n",
        "print('Number of training data:',len(train_x))\n",
        "print('Number of validation data:',len(val_x))\n",
        "\n",
        "print(train_x[1].shape)\n",
        "plt.imshow(train_x[1][:,:,0], cmap='gray')\n",
        "print('Label value: ',train_y[1])\n",
        "print('Raw Label value: ', orig_y[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training data: 2878\n",
            "Number of validation data: 323\n",
            "(32, 128, 1)\n",
            "Label value:  [27, 0, 11, 11, 14, 14, 13, 8, 18, 19, 18]\n",
            "Raw Label value:  Balloonists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nko7w/8feOjLnFBR1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwvHPaE4uQc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  \n",
        "# pad each output label to maximum text length\n",
        "# use \"post\" padding\n",
        "# this is not zero padding, we want to pad a specific value: len(char_list)\n",
        " \n",
        "train_padded_y = pad_sequences(train_y , maxlen = max_label_len,dtype = \"int32\",\n",
        "  padding = \"post\", value = len(char_list))\n",
        "val_padded_y = pad_sequences(val_y , maxlen = max_label_len,dtype = \"int32\",\n",
        "  padding = \"post\", value = len(char_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpY-hAJ0uQdB",
        "colab_type": "text"
      },
      "source": [
        "### Network Archtecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqtDrhSuQdC",
        "colab_type": "text"
      },
      "source": [
        "Paper link: (https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "1. Input shape (32,128,1)\n",
        "2. Use CNN to produce feature map\n",
        "5. Make feature map compatible with LSTM layer.\n",
        "6. Use two Bidirectional LSTM layers each of which has 128 units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYNY8LTFuQdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1), name='net_input')\n",
        " \n",
        "# Conv2D: 64 filters, (3,3) kernels, rectified unit, use \"same\" padding\n",
        "# Pooling: (2,2) size, stride 2\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer, \n",
        "# blog: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "# reduce the dimension\n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "# we want to return sequences, not the last output\n",
        "# use dropout 0.2\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True),input_shape=(32,128, 1))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True),input_shape=(32,128, 1))(blstm_1)\n",
        " \n",
        "# our final output has [len(char_list)+1] classes\n",
        "# we need to use softmax as the activation function\n",
        "num_classes = len(char_list)+1\n",
        "outputs = Dense(num_classes, activation='softmax')(blstm_2)\n",
        "\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8o6GBZjuQdF",
        "colab_type": "code",
        "outputId": "fcb4af7b-4611-4ed7-877b-b68d77c61506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "act_model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "net_input (InputLayer)       (None, 32, 128, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 31, 63)            16191     \n",
            "=================================================================\n",
            "Total params: 6,619,711\n",
            "Trainable params: 6,617,663\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbNPzexduQdH",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKCxvTfluQdI",
        "colab_type": "text"
      },
      "source": [
        "CTC Details: (https://theailearner.com/2019/05/29/connectionist-temporal-classificationctc/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRy_1ut8uQdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaQpfg-iuQdL",
        "colab_type": "text"
      },
      "source": [
        "### train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNFWj6lfuQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "# this is a keras functionality, after each epoch if we find the current model \n",
        "# has the lowest loss so far, we will save the model\n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwx0i3sQybiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07E4Pbn1uQdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert list to np array\n",
        "train_x = np.array(train_x)\n",
        "train_x_len = np.array(train_x_len)\n",
        "train_y_len = np.array(train_y_len)\n",
        "\n",
        "val_x = np.array(val_x)\n",
        "val_x_len = np.array(val_x_len)\n",
        "val_y_len = np.array(val_y_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FaAXakPuQdP",
        "colab_type": "code",
        "outputId": "97ce5d97-f9a8-48a1-b7c2-fde6c9868028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "# actually start the training\n",
        "model.fit(x=[train_x, train_padded_y, train_x_len, train_y_len], y=np.zeros(len(train_x)), batch_size=batch_size, epochs = epochs, validation_data = ([val_x, val_padded_y, val_x_len, val_y_len], [np.zeros(len(val_x))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2878 samples, validate on 323 samples\n",
            "Epoch 1/10\n",
            "2878/2878 [==============================] - 24s 8ms/step - loss: 43.5906 - val_loss: 31.2690\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 31.26899, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 31.8967 - val_loss: 30.8036\n",
            "\n",
            "Epoch 00002: val_loss improved from 31.26899 to 30.80361, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 31.4903 - val_loss: 30.6368\n",
            "\n",
            "Epoch 00003: val_loss improved from 30.80361 to 30.63685, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 31.2667 - val_loss: 30.3959\n",
            "\n",
            "Epoch 00004: val_loss improved from 30.63685 to 30.39592, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 31.0680 - val_loss: 30.1895\n",
            "\n",
            "Epoch 00005: val_loss improved from 30.39592 to 30.18946, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 30.9053 - val_loss: 30.0398\n",
            "\n",
            "Epoch 00006: val_loss improved from 30.18946 to 30.03982, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 30.7766 - val_loss: 29.9733\n",
            "\n",
            "Epoch 00007: val_loss improved from 30.03982 to 29.97332, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 30.6688 - val_loss: 29.8720\n",
            "\n",
            "Epoch 00008: val_loss improved from 29.97332 to 29.87195, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 30.5582 - val_loss: 29.7661\n",
            "\n",
            "Epoch 00009: val_loss improved from 29.87195 to 29.76615, saving model to best_model.hdf5\n",
            "Epoch 10/10\n",
            "2878/2878 [==============================] - 10s 3ms/step - loss: 30.4471 - val_loss: 29.8823\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 29.76615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabccf89978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYeN6RDuQdT",
        "colab_type": "text"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZL3d7XEuQdU",
        "colab_type": "code",
        "outputId": "e1118851-c749-4186-cea4-da89e3032691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(val_x[:10])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text =  \", val_orig_y[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original_text =   Incas\n",
            "predicted text = \n",
            "\n",
            "original_text =   Sufficing\n",
            "predicted text = \n",
            "\n",
            "original_text =   PETIT\n",
            "predicted text = \n",
            "\n",
            "original_text =   contagious\n",
            "predicted text = \n",
            "\n",
            "original_text =   MORRIS\n",
            "predicted text = \n",
            "\n",
            "original_text =   Fathead\n",
            "predicted text = \n",
            "\n",
            "original_text =   Lollies\n",
            "predicted text = \n",
            "\n",
            "original_text =   staider\n",
            "predicted text = \n",
            "\n",
            "original_text =   Puss\n",
            "predicted text = \n",
            "\n",
            "original_text =   Oars\n",
            "predicted text = \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zDmeF7ruQdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}