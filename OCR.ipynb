{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9OHEzwXIvmCL",
        "YRYeN6RDuQdT"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c3qian/Hello-Chao/blob/master/OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "code_folding": [],
        "id": "XFfgMJlPuQcn",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Recurrent Neural Network (CRNN) for OCR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JgNYfx7uQcq",
        "colab_type": "text"
      },
      "source": [
        "Steps for OCR:\n",
        "\n",
        "1. Preprocessing Data\n",
        "2. Creating Network Architecture(CTC loss function)\n",
        "3. Training Model\n",
        "4. Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHEzwXIvmCL",
        "colab_type": "text"
      },
      "source": [
        "###Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgFIw1auQcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "\n",
        "from glob import glob\n",
        " \n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpr242KuQc3",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9JmCRIVuQc6",
        "colab_type": "text"
      },
      "source": [
        "1. Download and unzip the dataset into a folder\n",
        "2. Preprocess the data: both inputs and outputs\n",
        "\n",
        "Input:\n",
        "\n",
        "*   Read the images and convert them into gray-scale images(why?)\n",
        "*   Reshape each image to size (128,32) (why?)\n",
        "*   Expand the dimension of the image from (128,32) to (128,32,1) (why?)\n",
        "*   Normalize the image pixel values by dividing it with 255\n",
        "\n",
        "Output:\n",
        "\n",
        "*   Read the image file names as the labels of that image\n",
        "*   Encode word into digits using a map (‘a’:0, ‘b’:1 …….. ‘z’:26 ......) e.g.  \"aabb\" -> [0,0,1,1]\n",
        "*   Find the maximum length among all words and pad every label to be the same size(max size) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJTiEllx815",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://transfer.sh/ZHMV4/dataset.zip\n",
        "!unzip dataset.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djKsVr35uQc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'dataset/'\n",
        " \n",
        "#lists for training dataset\n",
        "train_x = []\n",
        "train_y = []\n",
        "train_x_len = []\n",
        "train_y_len = []\n",
        "orig_y = []\n",
        " \n",
        "#lists for validation dataset\n",
        "val_x = []\n",
        "val_y = []\n",
        "val_x_len = []\n",
        "val_y_len = []\n",
        "val_orig_y = []\n",
        " \n",
        "max_label_len = 0\n",
        " \n",
        "flag = 0\n",
        " \n",
        "for i, f_name in enumerate(glob(os.path.join(path,'*/*.jpg'))):\n",
        "    # read input image and convert into gray scale image\n",
        "    img = ?\n",
        "\n",
        "    # convert each image of shape (32, 128, 1)\n",
        "    # hint: cv2.resize, np.expand_dims\n",
        "    img = ?\n",
        "\n",
        "    # Normalize each image\n",
        "    img = ?\n",
        "\n",
        "    # get the text from the image\n",
        "    txt = os.path.basename(f_name).split('_')[1]\n",
        "\n",
        "    # compute maximum length of the text\n",
        "    if len(txt) > max_label_len:\n",
        "        max_label_len = len(txt)\n",
        "\n",
        "\n",
        "    # split the data into validation and training dataset as 1:9\n",
        "    if ?:     \n",
        "        val_x.append(img)\n",
        "        val_y.append(encode_to_labels(txt))\n",
        "        val_x_len.append(31)\n",
        "        val_y_len.append(len(txt))\n",
        "        val_orig_y.append(txt)  \n",
        "    else:\n",
        "        train_x.append(img)\n",
        "        train_y.append(encode_to_labels(txt)) \n",
        "        train_x_len.append(31)\n",
        "        train_y_len.append(len(txt))\n",
        "        orig_y.append(txt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqOzYAS5Jhp",
        "colab_type": "code",
        "outputId": "762dd486-3b88-4c6e-bb3a-3a06c78a5b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# Check your preprocessing results\n",
        "print('Number of training data:',len(train_x))\n",
        "print('Number of validation data:',len(val_x))\n",
        "\n",
        "plt.imshow(train_x[0][:,:,0], cmap='gray')\n",
        "print('Label value: ',train_y[0])\n",
        "print('Raw Label value: ', orig_y[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training data: 2881\n",
            "number of validation data: 320\n",
            "Label value:  [38, 0, 17, 0, 1, 14, 20, 19]\n",
            "Raw Label value:  Marabout\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB2CAYAAADY3GjsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwvHPaE4uQc_",
        "colab_type": "code",
        "outputId": "78328825-9de9-4e41-b3c8-1b9ea62dda3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "char_list = string.ascii_letters+string.digits\n",
        "print('char_list:',char_list)\n",
        "print('total length:', len(char_list))\n",
        " \n",
        "# every word is encoded as a list of digits\n",
        "# the digit for each character is represented by the index\n",
        "# e.g. aabb -> [0,0,1,1], index of a is 0, index of b is 1\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    ?\n",
        "    return dig_lst\n",
        "  \n",
        "# pad each output label to maximum text length\n",
        "# use \"post\" padding\n",
        "# this is not zero padding, we want to pad a specific value: len(char_list)\n",
        " \n",
        "train_padded_y = pad_sequences(?)\n",
        "val_padded_y = pad_sequences(?)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_list: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "total length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpY-hAJ0uQdB",
        "colab_type": "text"
      },
      "source": [
        "### Network Archtecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnqtDrhSuQdC",
        "colab_type": "text"
      },
      "source": [
        "Paper link: (https://arxiv.org/pdf/1507.05717.pdf)\n",
        "\n",
        "1. Input shape (32,128,1)\n",
        "2. Use CNN to produce feature map\n",
        "5. Make feature map compatible with LSTM layer.\n",
        "6. Use two Bidirectional LSTM layers each of which has 128 units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYNY8LTFuQdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(?)\n",
        " \n",
        "# Conv2D: 64 filters, (3,3) kernels, rectified unit, use \"same\" padding\n",
        "# Pooling: (2,2) size, stride 2\n",
        "conv_1 = ?\n",
        "pool_1 = ?\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer, \n",
        "# blog: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "# reduce the dimension\n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "# we want to return sequences, not the last output\n",
        "# use dropout 0.2\n",
        "blstm_1 = Bidirectional(LSTM(?))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(?))(blstm_1)\n",
        " \n",
        "# our final output has [len(char_list)+1] classes\n",
        "# we need to use softmax as the activation function\n",
        "outputs = Dense(?)(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8o6GBZjuQdF",
        "colab_type": "code",
        "outputId": "96632a69-8701-4eab-cb49-248ffea5e4d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "act_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 32, 128, 1)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 32, 128, 64)       640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 16, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 16, 64, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 8, 32, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 8, 32, 256)        295168    \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 8, 32, 256)        590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 4, 32, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 4, 32, 512)        1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 4, 32, 512)        2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 32, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 2, 32, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 1, 31, 512)        1049088   \n",
            "_________________________________________________________________\n",
            "lambda_4 (Lambda)            (None, 31, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 31, 256)           656384    \n",
            "_________________________________________________________________\n",
            "bidirectional_8 (Bidirection (None, 31, 256)           394240    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 31, 63)            16191     \n",
            "=================================================================\n",
            "Total params: 6,619,711\n",
            "Trainable params: 6,617,663\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbNPzexduQdH",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKCxvTfluQdI",
        "colab_type": "text"
      },
      "source": [
        "CTC Details: (https://theailearner.com/2019/05/29/connectionist-temporal-classificationctc/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRy_1ut8uQdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#model to be used at training time\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaQpfg-iuQdL",
        "colab_type": "text"
      },
      "source": [
        "### train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNFWj6lfuQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "# this is a keras functionality, after each epoch if we find the current model \n",
        "# has the lowest loss so far, we will save the model\n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07E4Pbn1uQdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert list to np array\n",
        "train_x = np.array(train_x)\n",
        "train_x_len = np.array(train_x_len)\n",
        "train_y_len = np.array(train_y_len)\n",
        "\n",
        "val_x = np.array(val_x)\n",
        "val_x_len = np.array(val_x_len)\n",
        "val_y_len = np.array(val_y_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FaAXakPuQdP",
        "colab_type": "code",
        "outputId": "af2e7b75-be5b-4ac5-9379-50e65b25be2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "# actually start the training\n",
        "model.fit(x=[train_x, train_padded_y, train_x_len, train_y_len], y=np.zeros(len(train_x)), batch_size=batch_size, epochs = epochs, validation_data = ([val_x, val_padded_y, val_x_len, val_y_len], [np.zeros(len(val_x))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2881 samples, validate on 320 samples\n",
            "Epoch 1/10\n",
            "2881/2881 [==============================] - 9s 3ms/step - loss: 38.1213 - val_loss: 33.0928\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 33.09279, saving model to best_model.hdf5\n",
            "Epoch 2/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 31.8746 - val_loss: 32.3515\n",
            "\n",
            "Epoch 00002: val_loss improved from 33.09279 to 32.35153, saving model to best_model.hdf5\n",
            "Epoch 3/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 31.4135 - val_loss: 32.0724\n",
            "\n",
            "Epoch 00003: val_loss improved from 32.35153 to 32.07244, saving model to best_model.hdf5\n",
            "Epoch 4/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 31.1873 - val_loss: 31.8871\n",
            "\n",
            "Epoch 00004: val_loss improved from 32.07244 to 31.88713, saving model to best_model.hdf5\n",
            "Epoch 5/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 31.0068 - val_loss: 31.6960\n",
            "\n",
            "Epoch 00005: val_loss improved from 31.88713 to 31.69603, saving model to best_model.hdf5\n",
            "Epoch 6/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 30.8670 - val_loss: 31.5679\n",
            "\n",
            "Epoch 00006: val_loss improved from 31.69603 to 31.56786, saving model to best_model.hdf5\n",
            "Epoch 7/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 30.7500 - val_loss: 31.4827\n",
            "\n",
            "Epoch 00007: val_loss improved from 31.56786 to 31.48266, saving model to best_model.hdf5\n",
            "Epoch 8/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 30.6577 - val_loss: 31.4498\n",
            "\n",
            "Epoch 00008: val_loss improved from 31.48266 to 31.44982, saving model to best_model.hdf5\n",
            "Epoch 9/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 30.5709 - val_loss: 31.4793\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 31.44982\n",
            "Epoch 10/10\n",
            "2881/2881 [==============================] - 5s 2ms/step - loss: 30.4630 - val_loss: 31.3679\n",
            "\n",
            "Epoch 00010: val_loss improved from 31.44982 to 31.36793, saving model to best_model.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2def903e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYeN6RDuQdT",
        "colab_type": "text"
      },
      "source": [
        "### Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZL3d7XEuQdU",
        "colab_type": "code",
        "outputId": "5a93fddc-dd11-4da5-d2ca-17b3ce90b45f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(val_x[:10])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text =  \", val_orig_y[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 31, 63)\n",
            "original_text =   HONDURANS\n",
            "predicted text = \n",
            "\n",
            "original_text =   CONSULTANTS\n",
            "predicted text = \n",
            "\n",
            "original_text =   BLUCHER\n",
            "predicted text = \n",
            "\n",
            "original_text =   Skadden\n",
            "predicted text = \n",
            "\n",
            "original_text =   APULEIUS\n",
            "predicted text = \n",
            "\n",
            "original_text =   ambassadors\n",
            "predicted text = \n",
            "\n",
            "original_text =   THEREABOUT\n",
            "predicted text = \n",
            "\n",
            "original_text =   FARADIZING\n",
            "predicted text = \n",
            "\n",
            "original_text =   moralistic\n",
            "predicted text = \n",
            "\n",
            "original_text =   Managing\n",
            "predicted text = \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zDmeF7ruQdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}